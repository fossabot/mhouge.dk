@misc{clement_us_2022,
	title = {U.{S}. top live streaming platform 2021},
	url = {https://www.statista.com/statistics/1221858/most-popular-livestream-platform-watch-us/},
	howpublished = "\url{https://www.statista.com/statistics/1221858/most-popular-livestream-platform-watch-us/}",
	abstract = {As of February 2021, Twitch was the most popular platform to watch live streams on in the United States.},
	language = {en},
	urldate = {2022-11-07},
	journal = {Statista},
	author = {Clement, J.},
	month = mar,
	year = {2022},
}

@misc{twitch_twitch_2022,
	title = {Twitch {Chat} \& {Chatbots}},
	url = {https://dev.twitch.tv/docs/irc},
	howpublished = "\url{https://dev.twitch.tv/docs/irc}",
	abstract = {Twitch Chat \& Chatbots},
	language = {en},
	urldate = {2022-11-07},
	journal = {Twitch Developers},
	author = {{Twitch}},
	month = nov,
	year = {2022},
}

@misc{mozilla_websocket_nodate,
	title = {The {WebSocket} {API} ({WebSockets}) - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API},
	abstract = {The WebSocket API is an advanced technology that makes it possible to open a two-way interactive communication session between the user's browser and a server. With this API, you can send messages to a server and receive event-driven responses without having to poll the server for a reply.},
	language = {en-US},
	urldate = {2022-11-07},
	author = {{Mozilla}},
}

@misc{wikimedia_foundation_websocket_2022,
	title = {{WebSocket}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=WebSocket&oldid=1120212787},
	abstract = {WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011. The current API specification allowing web applications to use this protocol is known as WebSockets. It is a living standard maintained by the WHATWG and a successor to The WebSocket API from the W3C.WebSocket is distinct from HTTP. Both protocols are located at layer 7 in the OSI model and depend on TCP at layer 4. Although they are different, RFC 6455 states that WebSocket "is designed to work over HTTP ports 443 and 80 as well as to support HTTP proxies and intermediaries", thus making it compatible with HTTP. To achieve compatibility, the WebSocket handshake uses the HTTP Upgrade header to change from the HTTP protocol to the WebSocket protocol.
The WebSocket protocol enables interaction between a web browser (or other client application) and a web server with lower overhead than half-duplex alternatives such as HTTP polling, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the client without being first requested by the client, and allowing messages to be passed back and forth while keeping the connection open. In this way, a two-way ongoing conversation can take place between the client and the server. The communications are usually done over TCP port number 443 (or 80 in the case of unsecured connections), which is beneficial for environments that block non-web Internet connections using a firewall. Similar two-way browser–server communications have been achieved in non-standardized ways using stopgap technologies such as Comet or Adobe Flash Player.Most browsers support the protocol, including Google Chrome, Firefox, Microsoft Edge, Internet Explorer, Safari and Opera.Unlike HTTP, WebSocket provides full-duplex communication.
Additionally, WebSocket enables streams of messages on top of TCP. TCP alone deals with streams of bytes with no inherent concept of a message. Before WebSocket, port 80 full-duplex communication was attainable using Comet channels; however, Comet implementation is nontrivial, and due to the TCP handshake and HTTP header overhead, it is inefficient for small messages. The WebSocket protocol aims to solve these problems without compromising the security assumptions of the web.
The WebSocket protocol specification defines ws (WebSocket) and wss (WebSocket Secure) as two new uniform resource identifier (URI) schemes that are used for unencrypted and encrypted connections respectively. Apart from the scheme name and fragment (i.e. \# is not supported), the rest of the URI components are defined to use URI generic syntax.Using browser developer tools, developers can inspect the WebSocket handshake as well as the WebSocket frames.},
	language = {en},
	urldate = {2022-11-07},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = nov,
	year = {2022},
	note = {Page Version ID: 1120212787},
}

@misc{twitch_deprecation_2022,
	title = {Deprecation of chat commands through {IRC} - {Announcements}},
	url = {https://discuss.dev.twitch.tv/t/deprecation-of-chat-commands-through-irc/40486},
	abstract = {Twitch Chat and the Twitch API are two essential interfaces that third-party developers rely on to build innovative integrations for broadcasters and their viewers.  Historically, there has been an unfortunate segmentation of functionality across these two products.  Twitch Chat has included both the conversation as well as some actions that you would typically leverage via an API.  Until recently for example, developers had to rely on the /raid command through IRC to start a raid even if their ...},
	language = {en},
	urldate = {2022-11-09},
	journal = {Twitch Developer Forums},
	author = {{Twitch}},
	month = sep,
	year = {2022},
}

@misc{noauthor_optical_2020,
	title = {Optical character recognition},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://da.wikipedia.org/w/index.php?title=Optical_character_recognition&oldid=10435828},
	abstract = {Optical character recognition, ofte forkortet OCR, optisk tegngenkendelse, er den mekaniske eller tekniske konvertering af håndlavede, skrevne eller printede billeder med tekst, til maskinredigerbar tekst. Tidligere har man lettet den maskinelle genkendelse ved at benytte specielle skrifter, såsom OCR-A og OCR-B, til dokumenter, der senere skulle kunne læses maskinelt.
Dette bruges blandt andet af spambots til at læse CAPTCHA og af scannere til at danne en maskinlæsbar tekst, så man bl a kan søge og kopiere ord.},
	language = {da},
	urldate = {2022-11-10},
	journal = {Wikipedia, den frie encyklopædi},
	month = aug,
	year = {2020},
	note = {Page Version ID: 10435828},
}

@misc{wikimedia_foundation_optical_2022,
	title = {Optical character recognition},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Optical_character_recognition&oldid=1112090236},
	abstract = {Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast).Widely used as a form of data entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.
Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.},
	language = {en},
	urldate = {2022-11-10},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = sep,
	year = {2022},
	note = {Page Version ID: 1112090236},
}

@misc{wikimedia_foundation_object_2022,
	title = {Object detection},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Object_detection&oldid=1111857506},
	abstract = {Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.},
	language = {en},
	urldate = {2022-11-10},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = sep,
	year = {2022},
	note = {Page Version ID: 1111857506},
}

@misc{wikimedia_foundation_automatic_2022,
	title = {Automatic image annotation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Automatic_image_annotation&oldid=1119204667},
	abstract = {Automatic image annotation (also known as automatic image tagging or linguistic indexing) is the process by which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. This application of computer vision techniques is used in image retrieval systems to organize and locate images of interest from a database.
This method can be regarded as a type of multi-class image classification with a very large number of classes - as large as the vocabulary size. Typically, image analysis in the form of extracted feature vectors and the training annotation words are used by machine learning techniques to attempt to automatically apply annotations to new images. The first methods learned the correlations between image features and training annotations, then techniques were developed using machine translation to try to translate the textual vocabulary with the 'visual vocabulary', or clustered regions known as blobs. Work following these efforts have included classification approaches, relevance models and so on.
The advantages of automatic image annotation versus content-based image retrieval (CBIR) are that queries can be more naturally specified by the user. CBIR generally (at present) requires users to search by image concepts such as color and texture, or finding example queries. Certain image features in example images may override the concept that the user is really focusing on. The traditional methods of image retrieval such as those used by libraries have relied on manually annotated images, which is expensive and time-consuming, especially given the large and constantly growing image databases in existence.},
	language = {en},
	urldate = {2022-11-10},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1119204667},
}

@misc{wikimedia_foundation_neuro-linguistic_2022,
	title = {Neuro-linguistic programming},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Neuro-linguistic_programming&oldid=1117535406},
	abstract = {Neuro-linguistic programming (NLP) is a pseudoscientific approach to communication, personal development and psychotherapy, that first appeared in Richard Bandler and John Grinder's 1975 book The Structure of Magic I. NLP claims that there is a connection between neurological processes (neuro-), language (linguistic) and acquired behavioral patterns (programming), and that these can be changed to achieve specific goals in life.: 2  According to Bandler and Grinder, NLP can treat problems such as phobias, depression, tic disorders, psychosomatic illnesses, near-sightedness, allergy, the common cold, and learning disorders, often in a single session. They also claim that NLP can "model" the skills of exceptional people, allowing anyone to acquire them.: 5–6 NLP has been adopted by some hypnotherapists, as well as by companies that run seminars marketed as "leadership training" to businesses and government agencies.There is no scientific evidence supporting the claims made by NLP advocates, and it has been called as a pseudoscience. Scientific reviews have shown that NLP is based on outdated metaphors of for the brain's inner workings, that are inconsistent with current neurological theory and contain numerous factual errors. Reviews also found that research that favored NLP contained significant methodological flaws, and that there were three times as many studies of a much higher quality that failed to reproduce the "extraordinary claims" made by Bandler, Grinder, and other NLP practitioners.},
	language = {en},
	urldate = {2022-11-10},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1117535406},
	file = {Snapshot:/home/houge/Zotero/storage/FPZP9PN9/Neuro-linguistic_programming.html:text/html},
}

@misc{jaisen_mathai_cloud_nodate,
	title = {Cloud run vs cloud functions for serverless},
	url = {https://cloud.google.com/blog/products/serverless/cloud-run-vs-cloud-functions-for-serverless},
	abstract = {When building on top of a serverless platform like Cloud Run or Cloud Functions, here’s a framework for deciding which to choose for a given workload.},
	language = {en-US},
	urldate = {2022-11-16},
	journal = {Google Cloud Blog},
	author = {{Jaisen Mathai} and {Sara Ford}},
}

@misc{google_cloud_nodate,
	title = {Cloud {Functions} version comparison {\textbar} {Cloud} {Functions} {Documentation}},
	url = {https://cloud.google.com/functions/docs/concepts/version-comparison},
	language = {en},
	urldate = {2022-11-16},
	journal = {Google Cloud},
	author = {{Google}},
}

@misc{noauthor_pricing_nodate,
	title = {Pricing {\textbar} {Cloud} {Functions}},
	url = {https://cloud.google.com/functions/pricing},
	abstract = {Review pricing information for Cloud Functions},
	language = {en},
	urldate = {2022-11-16},
	journal = {Google Cloud},
}

@misc{blaquiere_cloud_2019,
	title = {Cloud {Run} {VS} {Cloud} {Functions}: {What}’s the lowest cost?},
	shorttitle = {Cloud {Run} {VS} {Cloud} {Functions}},
	url = {https://medium.com/google-cloud/cloud-run-vs-cloud-functions-whats-the-lowest-cost-728d59345a2e},
	abstract = {Cloud Run and Cloud Functions propose different serverless ways. But, above the technical differences, what’s the cheaper ?},
	language = {en},
	urldate = {2022-11-16},
	journal = {Google Cloud - Community},
	author = {blaquiere, guillaume},
	month = nov,
	year = {2019},
}

@misc{blaquiere_serverless_2021,
	title = {Serverless performance comparison: {Does} the language matter?},
	shorttitle = {Serverless performance comparison},
	url = {https://medium.com/google-cloud/serverless-performance-comparison-does-the-language-matter-c72a7191c799},
	abstract = {Can we improve the performances, or reduce the cost of an application by changing the languages? Have a look on that comparison.},
	language = {en},
	urldate = {2022-11-16},
	journal = {Google Cloud - Community},
	author = {blaquiere, guillaume},
	month = nov,
	year = {2021},
}

@incollection{malawski_benchmarking_2018,
	title = {Benchmarking {Heterogeneous} {Cloud} {Functions}},
	isbn = {978-3-319-75177-1},
	abstract = {Cloud Functions, often called Function-as-a-Service (FaaS), pioneered by AWS Lambda, are an increasingly popular method of running distributed applications. As in other cloud offerings, cloud functions are heterogeneous, due to different underlying hardware, runtime systems, as well as resource management and billing models. In this paper, we focus on performance evaluation of cloud functions, taking into account heterogeneity aspects. We developed a cloud function benchmarking framework, consisting of one suite based on Serverless Framework, and one based on HyperFlow. We deployed the CPU-intensive benchmarks: Mersenne Twister and Linpack, and evaluated all the major cloud function providers: AWS Lambda, Azure Functions, Google Cloud Functions and IBM OpenWhisk. We make our results available online and continuously updated. We report on the initial results of the performance evaluation and we discuss the discovered insights on the resource allocation policies.},
	author = {Malawski, Maciej and Figiela, Kamil and Gajek, Adam and Zima, Adam},
	month = feb,
	year = {2018},
	doi = {10.1007/978-3-319-75178-8_34},
	pages = {415--426},
}

 
@misc{amazon-standard-queue, 
    title="Amazon Stanfard Queues", 
    url={https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html}, 
    howpublished="\url{https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html}",
    year={2022},
	urldate = {2022-12-24},author={{Amazon}}
}
 

@misc{amazon_amazon_nodate,
	title = {Amazon {SQS} queue types - {Amazon} {Simple} {Queue} {Service}},
	url = {https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-types.html},
	urldate = {2022-12-24},
	author = {{Amazon}},
 }
@misc{wikimedia_foundation_fifo_2022,
	title = {{FIFO} (computing and electronics)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=FIFO_(computing_and_electronics)&oldid=1104188471},
	abstract = {In computing and in systems theory, FIFO is an acronym for first in, first out (the first in is the first out), a method for organizing the manipulation of a data structure (often, specifically a data buffer) where the oldest (first) entry, or "head" of the queue, is processed first.
Such processing is analogous to servicing people in a queue area on a first-come, first-served (FCFS) basis, i.e. in the same sequence in which they arrive at the queue's tail.
FCFS is also the jargon term for the FIFO operating system scheduling algorithm, which gives every process central processing unit (CPU) time in the order in which it is demanded. FIFO's opposite is LIFO, last-in-first-out, where the youngest entry or "top of the stack" is processed first. A priority queue is neither FIFO or LIFO but may adopt similar behaviour temporarily or by default. Queueing theory encompasses these methods for processing data structures, as well as interactions between strict-FIFO queues.},
	language = {en},
	urldate = {2022-12-24},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = aug,
	year = {2022},
	note = {Page Version ID: 1104188471},
}

@misc{yousuf_rabbitmq_2022,
	title = {{RabbitMQ} vs {SQS} - {What}'s the {Difference}? ({Pros} and {Cons})},
	shorttitle = {{RabbitMQ} vs {SQS} - {What}'s the {Difference}?},
	url = {https://cloudinfrastructureservices.co.uk/rabbitmq-vs-sqs-whats-the-difference/},
	abstract = {RabbitMQ vs SQS – What’s the Difference? (Pros and Cons). Many people search for the Difference between RabbitMQ vs SQS. As technology is rising day by},
	language = {en-GB},
	urldate = {2022-12-24},
	journal = {Cloud Infrastructure Services},
	author = {Yousuf, Farhan},
	month = sep,
	year = {2022},
	file = {Snapshot:/home/houge/Zotero/storage/HGXQME6J/rabbitmq-vs-sqs-whats-the-difference.html:text/html},
}

@misc{wikimedia_foundation_pay-as-you-use_2021,
	title = {Pay-as-you-use},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Pay-as-you-use&oldid=1036059552},
	abstract = {Pay-as-you-use (or pay-per-use) is a payment model in cloud computing that charges based on resource usage. The practice is similar to the utility bills (e.g. electricity), where only actually consumed resources are charged.
One major benefit of the pay-as-you-use method is that there are no wasted resources (that were reserved, but not consumed), which can be a source of significant losses for the companies. Users only pay for utilized capacities, rather than provisioning a chunk of resources that may or may not be used.},
	language = {en},
	urldate = {2022-12-24},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = jul,
	year = {2021},
	note = {Page Version ID: 1036059552},
	file = {Snapshot:/home/houge/Zotero/storage/UZ8DLT9Q/Pay-as-you-use.html:text/html},
}

@misc{cloudzero_horizontal_nodate,
	title = {Horizontal {Vs}. {Vertical} {Scaling}: {How} {Do} {They} {Compare}?},
	url = {https://www.cloudzero.com/blog/horizontal-vs-vertical-scaling},
	urldate = {2022-12-24},
	author = {{CloudZero}},
}

@misc{amazon_amazon_nodate-1,
	title = {Amazon {SQS} {Standard} queues - {Amazon} {Simple} {Queue} {Service}},year={2022},
 howpublished="\url{https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html}",
	url = {https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html},
	urldate = {2022-12-24},
	author = {{Amazon}},
	file = {Amazon SQS Standard queues - Amazon Simple Queue Service:/home/houge/Zotero/storage/4I7XBUB5/standard-queues.html:text/html},
}

@misc{streamelements_twitch_nodate,
	title = {Twitch {Emotes} {Tracking} {\textbar} {Chat} {Stats} by {StreamElements}},
	url = {https://streamelements.com/},
	abstract = {Description Chat Stats tracks emote usage on Twitch in real time. See which emotes are the most popular on every Twitch channel},
	language = {en},
	urldate = {2022-12-28},
	journal = {StreamElements},
	author = {StreamElements},
}

@article{kobs_emote-controlled_2020,
	title = {Emote-{Controlled}: {Obtaining} {Implicit} {Viewer} {Feedback} {Through} {Emote}-{Based} {Sentiment} {Analysis} on {Comments} of {Popular} {Twitch}.tv {Channels}},
	volume = {3},
	shorttitle = {Emote-{Controlled}},
	doi = {10.1145/3365523},
	abstract = {In recent years, streaming platforms for video games have seen increasingly large interest, as so-called esports have developed into a lucrative branch of business. Like for other sports, watching esports has become a new kind of entertainment medium, which is possible due to platforms that allow gamers to live stream their gameplay, the most popular platform being Twitch.tv. On these platforms, users can comment on streams in real time and thereby express their opinion about the events in the stream. Due to the popularity of Twitch.tv, this can be a valuable source of feedback for streamers aiming to improve their reception in a gaming-oriented audience. In this work, we explore the possibility of deriving feedback for video streams on Twitch.tv by analyzing the sentiment of live text comments made by stream viewers in highly active channels. Automatic sentiment analysis on these comments is a challenging task, as one can compare the language used in Twitch.tv with that used by an audience in a stadium, shouting as loud as possible in sometimes nonorganized ways. This language is very different from common English, mixing Internet slang and gaming-related language with abbreviations, intentional and unintentional grammatical and orthographic mistakes, and emoji-like images called emotes . Classic lexicon-based sentiment analysis techniques therefore fail when applied to Twitch comments.
To overcome the challenge posed by the nonstandard language, we propose two unsupervised lexicon-based approaches that make heavy use of the information encoded in emotes, as well as a weakly supervised neural network–based classifier trained on the lexicon-based outputs, which is supposed to help generalization to unknown words by use of domain-specific word embeddings. To enable better understanding of Twitch.tv comments, we analyze a large dataset of comments, uncovering specific properties of their language, and provide a smaller set of comments labeled with sentiment information by crowdsourcing.
We present two case studies showing the effectiveness of our methods in generating sentiment trajectories for events live streamed on Twitch.tv that correlate well with specific topics in the given stream. This allows for a new kind of implicit real-time feedback gathering for Twitch streamers and companies producing games or streaming content on Twitch.
We make our datasets and code publicly available for further research.},
	journal = {ACM Transactions on Social Computing},
	author = {Kobs, Konstantin and Zehe, Albin and Bernstetter, Armin and Chibane, Julian and Pfister, Jan and Tritscher, Julian and Hotho, Andreas},
	month = may,
	year = {2020},
	pages = {1--34},
}

@misc{pingdom_irc_nodate,
	title = {{IRC} is dead, long live {IRC}},
	url = {https://www.pingdom.com/blog/irc-is-dead-long-live-irc/},
	urldate = {2023-01-02},
	author = {{pingdom}},
	file = {IRC is dead, long live IRC:/home/houge/Zotero/storage/ZLXA9LE2/irc-is-dead-long-live-irc.html:text/html},
}

@inproceedings{murley_websocket_2021,
	address = {Ljubljana Slovenia},
	title = {{WebSocket} {Adoption} and the {Landscape} of the {Real}-{Time} {Web}},
	isbn = {978-1-4503-8312-7},
	howpublished="\url{https://dl.acm.org/doi/10.1145/3442381.3450063}",
	url = {https://dl.acm.org/doi/10.1145/3442381.3450063},
	doi = {10.1145/3442381.3450063},
	abstract = {Developers are increasingly deploying web applications which require real-time bidirectional updates, a use case which does not naturally align with the traditional client-server architecture of the web. Many solutions have arisen to address this need over the preceding decades, including HTTP polling, Server-Sent Events, and WebSockets. This paper investigates this ecosystem and reports on the prevalence, benefits, and drawbacks of these technologies, with a particular focus on the adoption of WebSockets. We crawl the Tranco Top 1 Million websites to build a dataset for studying real-time updates in the wild. We find that HTTP Polling remains significantly more common than WebSockets, and WebSocket adoption appears to have stagnated in the past two to three years. We investigate some of the possible reasons for this decrease in the rate of adoption, and we contrast the adoption process to that of other web technologies. Our findings further suggest that even when WebSockets are employed, the prescribed best practices for securing them are often disregarded. The dataset is made available in the hopes that it may help inform the development of future real-time solutions for the web.},
	language = {en},
	urldate = {2023-01-02},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {ACM},
	author = {Murley, Paul and Ma, Zane and Mason, Joshua and Bailey, Michael and Kharraz, Amin},
	month = apr,
	year = {2021},
	pages = {1192--1203},
	file = {Murley et al. - 2021 - WebSocket Adoption and the Landscape of the Real-T.pdf:/home/houge/Zotero/storage/WFNUYU2X/Murley et al. - 2021 - WebSocket Adoption and the Landscape of the Real-T.pdf:application/pdf},
}

@misc{go_guide_nodate,
	title = {A {Guide} to the {Go} {Garbage} {Collector} - {The} {Go} {Programming} {Language}},
	url = {https://go.dev/doc/gc-guide},
	howpublished = "\url{https://go.dev/doc/gc-guide}",
	language = {en},
	urldate = {2023-01-04},
	author = {{Go}}
 }
@misc{howarth_why_2020,
	title = {Why {Discord} is switching from {Go} to {Rust}},
	url = {https://discord.com/blog/why-discord-is-switching-from-go-to-rust},
	howpublished = "\url{https://discord.com/blog/why-discord-is-switching-from-go-to-rust}",
	language = {en},
	urldate = {2023-01-04},
	author = {Howarth, Jesse},
	month = feb,
	year = {2020},
}

@misc{streamelements_twitch_nodate-1,
	title = {Twitch {Emotes} {Tracking} {\textbar} {Chat} {Stats} by {StreamElements}},
	url = {https://streamelements.com/},
	abstract = {Description Chat Stats tracks emote usage on Twitch in real time. See which emotes are the most popular on every Twitch channel},
	language = {en},
	urldate = {2023-01-04},
	journal = {StreamElements},
	author = {StreamElements},
}

@misc{mongodb_time_nodate,
	title = {Time {Series} — {MongoDB} {Manual}},
	url = {https://www.mongodb.com/docs/manual/core/timeseries-collections/},
    howpublished	 = "\url{https://www.mongodb.com/docs/manual/core/timeseries-collections/}",
	abstract = {Time series, IOT, time series analysis, time series data, time series db},
	language = {en},
	urldate = {2023-01-04},year={2021},
	journal = {Time Series - MongoDB Manual},
	author = {{MongoDB}},
}

@misc{mongodb_timeseries-limitations,
	title = {Time {Series} {Collection} {Limitations} — {MongoDB} {Manual}},
	url = {https://www.mongodb.com/docs/manual/core/timeseries/timeseries-limitations/},
	howpublished="\url{https://www.mongodb.com/docs/manual/core/timeseries/timeseries-limitations/}" ,
	abstract = {Time Series, IOT},
	language = {en},
	urldate = {2023-01-04},
	journal = {Time Series Collection Limitations},
	author = {{MongoDB}},
 }
@misc{thomas_proactive_2019,
	title = {A proactive approach to more secure code – {Microsoft} {Security} {Response} {Center}},
	url = {https://msrc-blog.microsoft.com/2019/07/16/a-proactive-approach-to-more-secure-code/},
	howpublished = "\url{https://msrc-blog.microsoft.com/2019/07/16/a-proactive-approach-to-more-secure-code/}",
	language = {en-US},
	urldate = {2023-01-04},
	author = {Thomas, Gavin},
	month = jul,
	year = {2019},
 }
@misc{mulders_when_nodate,
	title = {When to use {Rust} and when to use {Go} - {LogRocket} {Blog}},
	url = {https://blog.logrocket.com/when-to-use-rust-when-to-use-golang/},
	urldate = {2023-01-04},
	author = {Mulders, Michiel},
	file = {When to use Rust and when to use Go - LogRocket Blog:/home/houge/Zotero/storage/XQ7EJADJ/when-to-use-rust-when-to-use-golang.html:text/html},
}

@misc{wikimedia_foundation_service-oriented_2023,
	title = {Service-oriented architecture},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Service-oriented_architecture&oldid=1130956575},
	abstract = {In software engineering, service-oriented architecture (SOA) is an architectural style that focuses on discrete services instead of a monolithic design. By consequence, it is also applied in the field of software design where services are provided to the other components by application components, through a communication protocol over a network. A service is a discrete unit of functionality that can be accessed remotely and acted upon and updated independently, such as retrieving a credit card statement online. SOA is also intended to be independent of vendors, products and technologies.Service orientation is a way of thinking in terms of services and service-based development and the outcomes of services.A service has four properties according to one of many definitions of SOA:
It logically represents a repeatable business activity with a specified outcome.
It is self-contained.
It is a black box for its consumers, meaning the consumer does not have to be aware of the service's inner workings.
It may be composed of other services.Different services can be used in conjunction as a service mesh to provide the functionality of a large software application, a principle SOA shares with modular programming. Service-oriented architecture integrates distributed, separately maintained and deployed software components. It is enabled by technologies and standards that facilitate components' communication and cooperation over a network, especially over an IP network.
SOA is related to the idea of an API (application programming interface), an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software.  An API can be thought of as the service, and the SOA the architecture that allows the service to operate.},
	language = {en},
	urldate = {2023-01-05},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1130956575},
	file = {Snapshot:/home/houge/Zotero/storage/U7U9QJ6G/Service-oriented_architecture.html:text/html},
}

@misc{wikimedia_foundation_loose_2023,
	title = {Loose coupling},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Loose_coupling&oldid=1131313244},
	abstract = {In computing and systems design, a loosely coupled system is one 

in which components are weakly associated (have breakable relationships) with each other, and thus changes in one component least affect existence or performance of another component.
in which each of its components has, or makes use of, little or no knowledge of the definitions of other separate components. Subareas include the coupling of classes, interfaces, data, and services. Loose coupling is the opposite of tight coupling.},
	language = {en},
	urldate = {2023-01-05},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = jan,
	year = {2023},
	note = {Page Version ID: 1131313244},
	file = {Snapshot:/home/houge/Zotero/storage/CCZZD5GJ/Loose_coupling.html:text/html},
}

@misc{wikimedia_foundation_event-driven_2022,
	title = {Event-driven architecture},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Event-driven_architecture&oldid=1115201196},
	abstract = {Event-driven architecture (EDA) is a software architecture paradigm promoting the production, detection, consumption of, and reaction to events.},
	language = {en},
	urldate = {2023-01-05},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = oct,
	year = {2022},
	note = {Page Version ID: 1115201196},
	file = {Snapshot:/home/houge/Zotero/storage/8XEJCI9Z/Event-driven_architecture.html:text/html},
}

@misc{martinekuan_web-queue-worker_nodate,
	title = {Web-{Queue}-{Worker} architecture style - {Azure} {Architecture} {Center}},
	url = {https://learn.microsoft.com/en-us/azure/architecture/guide/architecture-styles/web-queue-worker},
	abstract = {Learn about the benefits, challenges, and best practices for Web-Queue-Worker architectures on Azure.},
	language = {en-us},
	urldate = {2023-01-05},
	author = {martinekuan},
	file = {Snapshot:/home/houge/Zotero/storage/2FTDI9KN/web-queue-worker.html:text/html},
}

@misc{wikimedia_foundation_grpc_2022,
	title = {{gRPC}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=GRPC&oldid=1127576221},
	abstract = {gRPC (Google Remote Procedure Calls)  is a cross-platform open source high performance Remote Procedure Call (RPC) framework. gRPC was initially created by Google, which has used a single general-purpose RPC infrastructure called Stubby to connect the large number of microservices running within and across its data centers for over a decade. In March 2015, Google decided to build the next version of Stubby and make it open source. The result was gRPC, which is now used in many organizations outside of Google to power use cases from microservices to the “last mile” of computing (mobile, web, and Internet of Things). It uses HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features such as authentication, bidirectional streaming and flow control, blocking or nonblocking bindings, and cancellation and timeouts. It generates cross-platform client and server bindings for many languages. Most common usage scenarios include connecting services in a microservices style architecture, or connecting mobile device clients to backend services.gRPC's complex use of HTTP/2 makes it impossible to implement a gRPC client in the browser, instead requiring a proxy.},
	language = {en},
	urldate = {2023-01-05},
	journal = {Wikipedia},
	author = {{Wikimedia Foundation}},
	month = dec,
	year = {2022},
	note = {Page Version ID: 1127576221},
	file = {Snapshot:/home/houge/Zotero/storage/TNWJC3DX/GRPC.html:text/html},
}

@misc{gyori_grpc_nodate,
	title = {{gRPC} vs. {REST}: {Get} {Started} {With} the {Best} {Protocol}},
	shorttitle = {{gRPC} vs. {REST}},
	url = {https://www.toptal.com/grpc/grpc-vs-rest-api},
	abstract = {Discover gRPC's fresh, new approach to web communication.},
	language = {en},
	urldate = {2023-01-05},
	journal = {Toptal Engineering Blog},
	author = {Gyori, Laszlo},
}

@misc{amazon_what_nodate,
	title = {What is a {Message} {Queue}?},
	url = {https://aws.amazon.com/message-queue/},
	abstract = {Learn about message queues, including features, benefits, how they can help decouple systems, and how to get started with message queues on AWS.},
	language = {en-US},
	urldate = {2023-01-05},
	journal = {Amazon Web Services, Inc.},
	author = {{Amazon}},
}

@misc{kobenhavns_universitet_klassisk_nodate,
	title = {Klassisk brainstorm – {Værktøjskassen} til innovation og entreprenørskab i undervisningen},
	url = {https://innovation.sites.ku.dk/metode/klassisk-brainstormi/},
	howpublished  = "\url{https://innovation.sites.ku.dk/metode/klassisk-brainstormi/}",
	language = {da-DK},
	urldate = {2023-01-05},year={2018},
	author = {{Københavns Universitet}},
}

@misc{kobenhavns_universitet_hurtige_nodate,
	title = {Hurtige prototyper (rapid prototyping) – {Værktøjskassen} til innovation og entreprenørskab i undervisningen},
	url = {https://innovation.sites.ku.dk/metode/hurtige-prototyper/},
	howpublished = "\url{https://innovation.sites.ku.dk/metode/hurtige-prototyper/}",
	language = {da-DK},
	urldate = {2023-01-05},year={2018},
	author = {{Københavns Universitet}}
}

@techreport{oikarinen_internet_1993,
	type = {Request for {Comments}},
	title = {Internet {Relay} {Chat} {Protocol}},
	url = {https://datatracker.ietf.org/doc/rfc1459},
	abstract = {The IRC protocol is a text-based protocol, with the simplest client being any socket program capable of connecting to the server. This memo defines an Experimental Protocol for the Internet community.},
	number = {RFC 1459},
	urldate = {2023-01-05},
	institution = {Internet Engineering Task Force},
	author = {Oikarinen, J. and Reed, D.},
	month = may,
	year = {1993},
	doi = {10.17487/RFC1459},
	note = {Num Pages: 65},
	file = {Full Text PDF:/home/houge/Zotero/storage/4Z7UKDRX/1993 - Internet Relay Chat Protocol.pdf:application/pdf},
}

@misc{caniuse_websockets, 
author={Caniuse},
	title = {{WebSocket} {API} {\textbar} {Can} {I} use... {Support} tables for {HTML5}, {CSS3}, etc},
	url = {https://caniuse.com/mdn-api_websocket},
	   howpublished = "\url{https://caniuse.com/mdn-api_websocket}",
    year = {2023},
	urldate = {2023-01-05},
}

 
@misc{cratesio_tungstenite_nodate,
	title = {tungstenite - crates.io: {Rust} {Package} {Registry}},
	url = {https://crates.io/crates/tungstenite},
	howpublished = "\url{https://crates.io/crates/tungstenite}",
	urldate = {2023-01-05},
 year = {2023},
	author = {{crates.io}},
 }
@misc{cratesio_tungstenite_nodate-1,
	title = {tungstenite - crates.io: {Rust} {Package} {Registry}},
	url = {https://crates.io/crates/tungstenite},
 	howpublished = "\url{https://crates.io/crates/tungstenite}",
	urldate = {2023-01-05},
	author = {{crates.io}},year = {2023},
}

@misc{weiss_irc_2022,
	title = {the irc crate},
	copyright = {MPL-2.0},
	url = {https://github.com/aatxe/irc},
	abstract = {the irc crate – usable, async IRC for Rust},
	urldate = {2023-01-05},
	author = {Weiss, Aaron},
	month = dec,
	year = {2022},
	note = {original-date: 2014-09-10T20:22:14Z},
	keywords = {async, client, irc, ircv3, protocol, thread-safe},
}

@misc{cratesio_irc_nodate,
	title = {irc - crates.io: {Rust} {Package} {Registry}},
	url = {https://crates.io/crates/irc}, 	
 howpublished = "\url{https://crates.io/crates/irc}",
	urldate = {2023-01-05},
	author = {{crates.io}},
 year = {2023},
}

@misc{amazon_sqs_queue_types,
	title = {Amazon {SQS} queue types - {Amazon} {Simple} {Queue} {Service}},
	url = {https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-types.html},
 howpublished="\url{https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-queue-types.html}",
	urldate = {2023-01-05},year={2022},
	author = {{Amazon}},
 }
@misc{amazon_amazon_nodate-3,
	title = {Amazon {SQS} {FIFO} ({First}-{In}-{First}-{Out}) queues - {Amazon} {Simple} {Queue} {Service}},
	url = {https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html},
	urldate = {2023-01-05},
	author = {{Amazon}},
 }
@misc{next_api_nodate,
	type = {Documentation},
	title = {{API} {Routes}: {Introduction} {\textbar} {Next}.js},
	shorttitle = {{API} {Routes}},
	howpublished = "\url{https://nextjs.org/docs/api-routes/introduction}",
	url = {https://nextjs.org/docs/api-routes/introduction},
	abstract = {Next.js supports API Routes, which allow you to build your API without leaving your Next.js app. Learn how it works here.},
	language = {en},
	urldate = {2023-01-05},
	journal = {API Routes: Introduction {\textbar} Next.js},
	author = {{Next}},
}

@misc{cloudflare_what_nodate,
	title = {What is serverless computing? {\textbar} {Serverless} definition {\textbar} {Cloudflare}},
	url = {https://www.cloudflare.com/learning/serverless/what-is-serverless/},
	howpublished = "\url{https://www.cloudflare.com/learning/serverless/what-is-serverless/}",
	urldate = {2023-01-06},
	author = {{Cloudflare}},
 
}

@misc{cloudflare_what_nodate-1,
	title = {What is serverless computing? {\textbar} {Serverless} definition},
	shorttitle = {What is serverless computing?},
	url = {https://www.cloudflare.com/learning/serverless/what-is-serverless/},
	abstract = {Learn about what serverless computing is, and how FaaS enables developers to write and deploy code in a serverless architecture.},
	language = {en-us},
	urldate = {2023-01-06},
	journal = {Cloudflare},
	author = {{Cloudflare}},
	file = {Snapshot:/home/houge/Zotero/storage/FFQIHHP7/what-is-serverless.html:text/html},
}

@misc{amazon_using_sdk,
	title = {Using the {AWS} {SDK} for {Rust} in {AWS} {Lambda} - {AWS} {SDK} for {Rust}},
	url = {https://docs.aws.amazon.com/sdk-for-rust/latest/dg/lambda.html},
	urldate = {2023-01-06},
	author = {{Amazon}},year={2022},
}

@misc{crunchbase_athenascope_nodate,
	title = {Athenascope - {Crunchbase} {Company} {Profile} \& {Funding}},
	url = {https://www.crunchbase.com/organization/athenascope},
	abstract = {Athenascope is a small startup aiming to tap computer vision intelligence to record, review and recap.},
	language = {en},
	urldate = {2023-01-06},
	journal = {Crunchbase},
 year = {2019},
	author = {{Crunchbase}},
	file = {Snapshot:/home/houge/Zotero/storage/SZEDVLBB/athenascope.html:text/html},
}

@misc{fung_chapter_nodate,
	title = {Chapter 40. {Computer} {Vision} on the {GPU}},
	url = {https://developer.nvidia.com/gpugems/gpugems2/part-v-image-oriented-computing/chapter-40-computer-vision-gpu},
	abstract = {Chapter 40. Computer Vision on the GPU James Fung University of Toronto Computer vision tasks are computationally intensive and repetitive, and they often exceed the real-time capabilities of the CPU, leaving little time for higher-level tasks. However, many computer vision operations map efficiently onto the modern GPU, whose programmability allows a wide variety of computer vision algorithms to be implemented. This chapter presents methods of efficiently mapping common mathematical operations of computer vision onto modern computer graphics architecture. 40.1 Introduction In some sense, computer graphics and computer vision are the opposites of one another. Computer graphics takes a numerical description of a scene and renders an image, whereas computer vision analyzes images to create numerical representations of the scene. Thus, carrying out computer vision tasks in graphics hardware uses the graphics hardware in an "inverse" fashion. The GPU provides a streaming, data-parallel arithmetic architecture. This type of architecture carries out a similar set of calculations on an array of image data. The single-instruction, multiple-data (SIMD) capability of the GPU makes it suitable for running computer vision tasks, which often involve similar calculations operating on an entire image. Special-purpose computer vision hardware is rarely found in typical mass-produced personal computers. Instead, the CPU is usually used for computer vision tasks. Optimized computer vision libraries for the CPU often consume many of the CPU cycles to achieve real-time performance, leaving little time for other, nonvision tasks. GPUs, on the other hand, are found on most personal computers and often exceed the capabilities of the CPU. Thus, we can use the GPU to accelerate computer vision computation and free up the CPU for other tasks. Furthermore, multiple GPUs can be used on the same machine, creating an architecture capable of running multiple computer vision algorithms in parallel. 40.2 Implementation Framework Many computer vision operations can be considered sequences of filtering operations, with each sequential filtering stage acting upon the output of the previous filtering stage. On the GPU, these filtering operations are carried out by fragment programs. To apply these fragment programs to input images, the input images are initialized as textures and then mapped onto quadrilaterals. By displaying these quadrilaterals in appropriately sized windows, we can ensure that there is a one-to-one correspondence of image pixels to output fragments. When the textured quadrilateral is displayed, the fragment program then runs, operating identically on each pixel of the image. The fragment program is analogous to the body of a common for-loop program statement, but each iteration of the body can be thought of as executing in parallel. The resulting output is not the original image, but rather the filtered image. Modern GPUs allow input and results in full 32-bit IEEE floating-point precision, providing sufficient accuracy for many algorithms. A complete computer vision algorithm can be created by implementing sequences of these filtering operations. After the texture has been filtered by the fragment program, the resulting image is placed into texture memory, either by using render-to-texture extensions or by copying the frame buffer into texture memory. In this way, the output image becomes the input texture to the next fragment program. This creates a pipeline that runs the entire computer vision algorithm. However, often a complete vision algorithm will require operations beyond filtering. For example, summations are common operations; we present examples of how they can be implemented and used. Furthermore, more-generalized calculations, such as feature tracking, can also be mapped effectively onto graphics hardware. These methods are implemented in the OpenVIDIA GPU computer vision library, which can be used to create real-time computer vision programs (http://openvidia.sourceforge.net). 40.3 Application Examples This chapter presents examples that implement common computer vision techniques on the GPU. The common thread between each of these algorithms is that they are all used in our system to create a fully interactive, real-time computer-mediated reality, in which the environment of a user appears to be augmented, diminished, or otherwise altered. Some examples are shown in Figure 40-1. Figure 40-1 Using Computer Vision to Mediate Reality 40.3.1 Using Sequences of Fragment Programs for Computer Vision In this section, we demonstrate how a sequence of fragment programs can work together to perform more-complex computer vision operations. Correcting Radial Distortion Many camera lenses cause some sort of radial distortion (also known as barrel distortion) of the image, most commonly in wide-field-of-view or low-cost lenses. There are several ways of correcting for radial distortion on the graphics hardware; we present an example of how a fragment program can be used to do so. We begin by assuming that the displacement of a pixel due to radial distortion is x, y and is commonly modeled by: where x, y are the image coordinates, r is the distance of the pixel from the principal point of the camera, and the coefficients k 1,2,3... are camera parameters determined by some known calibration method. Given a particular camera, the required calibration parameters can be found using the tools provided at Bouguet 2004. The fragment program shown in Listing 40-1 applies this correction by using the current texture coordinates to calculate an offset. The program then adds this offset to the current pixel's coordinates and uses the result to look up the texture value at the corrected coordinate. This texture value is then output, and the image's radial distortion is thereby corrected. Example 40-1. Radial Undistortion float4 CorrectBarrelDistortion( float2 fptexCoord : TEXCOORD0, float4 wpos : WPOS, uniform samplerRECT FPE1 ) : COLOR \{ // known constants for the particular lens and sensor float f = 368.28488; // focal length float ox = 147.54834; // principal point, x axis float oy = 126.01673; // principal point, y axis float k1 = 0.4142; // constant for radial distortion correction float k2 = 0.40348; float2 xy = (fptexCoord - float2(ox, oy))/float2(f,f); r = sqrt( dot(xy, xy) ); float r2 = r * r; float r4 = r2 * r2; float coeff = (k1 * r2 + k2 * r4); // add the calculated offsets to the current texture coordinates xy = ((xy + xy * coeff.xx) * f.xx) + float2(ox, oy); // look up the texture at the corrected coordinates // and output the color return texRECT(FPE1, xy); \} A Canny Edge Detector Edge detection is a key algorithm used in many vision applications. Here we present an implementation of the commonly used "Canny" edge-detection algorithm that runs entirely on the GPU. See Figure 40-2 for an example. We implement the Canny edge detector as a series of fragment programs, each performing a step of the algorithm: Step 1. Filter the image using a separable Gaussian edge filter (Jargstorff 2004). The filter footprint is typically around 15x15 elements wide, and separable. A separable filter is one whose 2D mask can be calculated by applying two 1D filters in the x and y directions. The separability saves us a large number of texture lookups, at the cost of an additional pass. Step 2. Determine the magnitude, l, of the derivatives at each pixel computed in step 1, and quantize the direction d. The values of d and l are given by: where d is the direction of the gradient vector, quantized to one of the eight possible directions one may traverse a 3x3 region of pixels. The length l is the magnitude of the vector. Step 3. Perform a nonmaximal suppression, as shown in Listing 40-2. The nonmaximal suppression uses the direction of the local gradient to determine which pixels are in the forward and backward direction of the gradient. This direction is used to calculate the x and y offset of the texture coordinates to look up the forward and backward neighbors. An edge is considered found only if the gradient is at a maximum as compared to its neighbors, ensuring a thin line along the detected edge. Figure 40-2 Canny Edge Detection Example 40-2. A Canny Edge Detector Using Nonmaximal Suppression This program looks up the direction of the gradient at the current pixel and then retrieves the pixels in the forward and backward directions along the gradient. The program returns 1 if the current pixel value is greater than the two values along the gradient; otherwise, it suppresses (or zeroes) the output. A thresholding value allows the sensitivity of the edge detector to be varied in real time to suit the vision apparatus and environment. float4 CannySearch( uniform float4 thresh, float2 fptexCoord : TEXCOORD0, uniform samplerRECT FPE1 ) : COLOR \{ // magdir holds \{ dx, dy, mag, direct \} float4 magdir = texRECT(FPE1, fptexCoord); float alpha = 0.5/sin(3.14159/8); // eight directions on grid float2 offset = round( alpha.xx * magdir.xy/magdir.zz ); float4 fwdneighbour, backneighbour; fwdneighbour = texRECT(FPE1, fptexCoord + offset ); backneighbour = texRECT(FPE1, fptexCoord + offset ); float4 colorO; if ( fwdneighbour.z {\textgreater} magdir.z {\textbar}{\textbar} backneighbour.z {\textgreater} magdir.z ) colorO = float4(0.0, 0.0, 0.0, 0.0); // not an edgel else colorO = float4(1.0, 1.0, 1.0, 1.0); // is an edgel if ( magdir.z {\textless} thresh.x ) colorO = float4(0.0, 0.0, 0.0, 0.0); // thresholding return colorO; \} 40.3.2 Summation Operations Many common vision operations involve the summation of an image buffer. Some methods that require the summation are moment calculations and the solutions to linear systems of equations. Buck and Purcell (2004) discuss a general method of efficiently summing buffers by using local neighborhoods. On systems that do not support render-to-texture, however, the number of passes required by this technique can be limiting, due to the number of graphics context switches involved. We have found performing the computation in two passes to be more efficient on such systems. In our approach, we create a fragment program that sums each row of pixels, resulting in a single column holding the sums of each row. Then, in a second pass, we draw a single pixel and execute a program that sums down the column. The result is a single pixel that has as its "color" the sum of the entire buffer. The Cg code for computing the sum in this way is given in this chapter's appendix on the book's CD. We use the summation operation within the context of two different, but commonly used, algorithms that require summation over a buffer: a hand-tracking algorithm and an algorithm for creating panoramas from images by determining how to stitch them together. Tracking Hands Tracking a user's hands or face is a useful vision tool for creating interactive systems, and it can be achieved on the GPU by using a combination of image filtering and moment calculations. A common way to track a user's hand is to use color segmentation to find areas that are skin-colored, and then continually track the location of those areas through the image. Additionally, it is useful to track the average color of the skin tone, because it changes from frame to frame, typically due to changes in lighting. It is common to carry out the segmentation in an HSV color space, because skin tone of all hues tends to cluster in HSV color space. The HSV color space describes colors in terms of their hue (the type of color, such as red, green, or blue); saturation (the vibrancy of that color); and value (the brightness of the color). Cameras typically produce an RGB image, so a fragment program can be used to perform a fast color space conversion as a filtering operation. Assuming that we have some sort of initial guess for the HSV color of the skin, we can also use a fragment program to threshold pixels. Given a particular pixel's HSV value, we can determine its difference from the mean HSV skin color and either output to 0 if the difference is too great or output the HSV value otherwise. For instance, the comparison might be easily done as: if ( distance( hsv, meanhsv ) {\textless} thresh ) \{ colorO = hsv; \} else colorO = float4(0.0, 0.0, 0.0, 0.0); Finally, we determine the centroid of the skin-colored region and update the mean skin value based on what is currently being seen. For both of these steps, we need to calculate sums over the image. To compute centroids, we can first compute the first three moments of the image: where W and H are the sizes of the image in each axis, and E (x, y) is the pixel value at location (x, y). The x coordinate of the centroid is then computed as M 10/M 00, and M 01/M 00 is the centroid's y coordinate. These summations can be done by first running a fragment program that fills a buffer with: and then using a GPU summation method to sum the entire buffer. The result will be a single pixel whose value can then be read back to the CPU to update the location of the centroid. In our case—tracking the centroid of a thresholded skin-tone blob—we might simply let the fragment program output 1.0 in skin-colored areas and 0 elsewhere. Listing 40-3 shows a Cg program that produces four output components that, after summation, will provide the sums needed to calculate the zeroth and first-order moments. Figure 40-3 depicts this process. Figure 40-3 Tracking a User's Hand Should we require more statistics like the preceding, we could either reduce the precision and output eight 16-bit floating-point results to a 128-bit floating-point texture or use the multiple render targets (MRTs) OpenGL extension. Example 40-3. Creating Output Statistics for Moment Calculations float4 ComputeMoments( float2 texCoord : TEXCOORD0, uniform samplerRECT FPE1: TEXUNIT0 ) : COLOR \{ float4 samp = texRECT(FPE1, fptexCoord); float4 colorO = float4(0, 0, 0, 0); if ( samp.x != 0.0 ) colorO = float4( 1.0, fptexCoord.x, fptexCoord.y, samp.y); \} 40.3.3 Systems of Equations for Creating Image Panoramas We also encounter this compute-and-sum type of calculation when we form systems of equations of the familiar form Ax = b. VideoOrbits VideoOrbits is an image-registration algorithm that calculates a coordinate transformation between pairs of images taken with a camera that is free to pan, tilt, rotate about its optical axis, and zoom. The technique solves the problem for two cases: (1) images taken from the same location of an arbitrary 3D scene or (2) images taken from arbitrary locations of a flat scene. Figure 40-4 shows an image panorama composed from several frames of video. Equivalently, Mann and Fung (2002) show how the same algorithm can be used to replace planes in the scene with computer-generated content, such as in Figure 40-1. Figure 40-4 A VideoOrbits Panorama VideoOrbits solves for a projection of an image in order to register it with another image. Consider a pair of images of the same scene, with the camera having moved between images. A given point in the scene appearing at coordinates [x, y] in the first image now appears at coordinates [x', y'] in the second image. Under the constraints mentioned earlier, all the points in the image can be considered to move according to eight parameters, given by the following equation: Equation 1 The eight scalar parameters describing the projection are denoted by: and are calculated by VideoOrbits. Once the parameters p have been determined for each image, the images can then be transformed into the same coordinate system to create the image panoramas of Figure 40-4. (For a more in-depth treatment of projective flow and the VideoOrbits algorithm, see Mann 1998.) VideoOrbits calculates the parameters p by solving a linear system of equations. The bulk of the computation is in the initialization of this system of equations, which is of the form Ax = b. Here, A is an 8x8 matrix whose values we must initialize, x is an 8x1 column vector that corresponds to p, and b is an 8x1 column vector we must also initialize. Solving this system is computationally expensive because each element of A and b is actually a summation of a series of multiplications and additions at each point in the image. Initializing each of the 64 entries of the matrix A involves many multiplications and additions over all the pixels. A is of the form: Equation 2 where each of the elements is a summation. (Note that the elements a 11, a 12, a 21, a 22 are not those of Equation 1.) The arguments of these summations involve many arithmetic operations. For instance, element a 21 is: Equation 3 This operation must be carried out at every pixel and then summed to obtain each entry in matrix A. This compute-and-sum operation is similar to moment calculations discussed previously. Not surprisingly, we can calculate this on the GPU in a similar fashion, filling a buffer with the argument of the summation and then summing the buffer. Typically, to calculate all the matrix entries, multiple passes will be required. As each summation is completed, the results are copied to a texture that stores the results. After all the entries for the matrix A and vector b are computed, the values are read back to the CPU. This keeps the operations on the GPU, deferring the readback until all matrix entries have been calculated. This allows the GPU to operate with greater independence from the CPU. The system can then be solved on the CPU, with the majority of the per-pixel calculations already solved by the GPU. 40.3.4 Feature Vector Computations Calculating feature vectors at detected feature points is a common operation in computer vision. A feature in an image is a local area around a point with some higher-than-average amount of "uniqueness." This makes the point easier to recognize in subsequent frames of video. The uniqueness of the point is characterized by computing a feature vector for each feature point. Feature vectors can be used to recognize the same point in different images and can be extended to more generalized object recognition techniques. Figure 40-5 shows the motion of the features between frames as the camera moves. Figure 40-5 Detecting Distinctive Points as the Camera Pans Computing feature vectors on the GPU requires handling the image data in a different manner than the operations we have thus far presented. Because only a few points in the image are feature points, this calculation does not need to be done at every point in the image. Additionally, for a single feature point, a large feature vector (100 elements or more) is typically calculated. Thus, each single point produces a larger number of results, rather than the one-to-one or many-to-one mappings we have discussed so far. These types of computations can still be carried out on the GPU. We take as an example a feature vector that is composed of the filtered gradient magnitudes in a local region around a located feature point. Feature detection can be achieved using methods similar to the Canny edge detector that instead search for corners rather than lines. If the feature points are being detected using sequences of filtering, as is common, we can perform the filtering on the GPU and read back to the CPU a buffer that flags which pixels are feature points. The CPU can then quickly scan the buffer to locate each of the feature points, creating a list of image locations at which we will calculate feature vectors on the GPU. There are many approaches to generating feature vectors in order to track feature points. We show one approach as an example. For each detected feature point, we examine a 16x16 neighborhood of gradient magnitudes and directions (as computed in the Canny edge example). This neighborhood is divided up into 4x4-pixel regions, giving 16 different regions of 16 pixels. In each region, we create an 8-element histogram of gradient magnitudes. Each pixel's gradient magnitude is added to the bin of the histogram corresponding to its quantized gradient direction. For each region, the result is then an 8-element histogram. The feature vector is the concatenation of all 16 of these 8-element histograms, giving a feature vector length of 16 x 8 = 128 elements. Features in different images can be matched to each other by computing the Euclidean distance between pairs of feature vectors, with the minimum distance providing a best match. To produce the 128-element feature vector result, we draw rows of vertices, with one row for each feature, such that each drawn point in the row corresponds to an element of the feature vector for that feature. Associating texture values pointwise allows for the most flexibility in mapping texture coordinates for use in computing the feature vector. Figure 40-6 shows how the vertices are laid out, and the binding of the image texture coordinates. Each point has different texture coordinates mapped to it to allow a single fragment program to access the appropriate gradients of the local region used to calculate the histogram. Figure 40-6 OpenGL Vertex Layout and Texture Bindings for Feature Vertices By associating the correct texture coordinates for each point vertex, the single fragment program shown in Listing 40-4 calculates the feature vectors. To produce 8 elements per region, we have packed the results into 16-bit half-precision floating-point values; alternatively, we could use MRTs to output multiple 32-bit values. After completion, we read back the frame buffer, containing one feature vector per row, to the CPU. Example 40-4. Fragment Program for Calculating Feature Vectors float4 FeatureVector( float2 fptexCoord : TEXCOORD0, float2 tapsCoord : TEXCOORD1, float4 colorZero : COLOR0, float4 wpos : WPOS, uniform samplerRECT FPE1, uniform samplerRECT FPE2 ) : COLOR \{\vphantom{\}} const float4x4 lclGauss = \{ 0.3536, 0.3536, 0.3536, 0.3536, 0.3536, 0.7071, 0.7071, 0.3536, 0.3536, 0.7071, 0.7071, 0.3536, 0.3536, 0.3536, 0.3536, 0.3536 \}; const float eps = 0.001; float4 input; int xoffset = 1, yoffset = 1; float4 bins\_0 = float4( 0.0, 0.0, 0.0, 0.0 ); float4 bins\_1 = float4( 0.0, 0.0, 0.0, 0.0 ); for( xoffset = 0 ; xoffset {\textless} 4 ; xoffset++ ) \{\vphantom{\}} for ( yoffset = 0 ; yoffset {\textless} 4 ; yoffset++ ) \{\vphantom{\}} input = texRECT( FPE1, fptexCoord + float2(xoffset, yoffset)); float dir = input.y; input *= lclGauss[xoffset,yoffset].xxxx * texRECT( FPE2, tapsCoord + float2(xoffset, yoffset)).xxxx; if ( dir},
	language = {en-US},
	urldate = {2023-01-06},
	journal = {NVIDIA Developer},
	author = {Fung, James},
 }
@misc{the_indian_express_openais_2022,
	title = {{OpenAI}’s {ChatGPT} chatbot: {Here}’s how much it costs to run per day},
	shorttitle = {{OpenAI}’s {ChatGPT} chatbot},
	url = {https://indianexpress.com/article/technology/tech-news-technology/chatgpt-interesting-things-to-know-8334991/},
	abstract = {It might cost around \$3 million per month for OpenAI to run ChatGPT.},
	language = {en},
	urldate = {2023-01-06},
	journal = {The Indian Express},
	author = {{The Indian Express}},
	month = dec,
	year = {2022},
}

@misc{meserole_how_2022,
	title = {How do recommender systems work on digital platforms?},
	url = {https://www.brookings.edu/techstream/how-do-recommender-systems-work-on-digital-platforms-social-media-recommendation-algorithms/},
 howpublished="\url{https://www.brookings.edu/techstream/how-do-recommender-systems-work-on-digital-platforms-social-media-recommendation-algorithms}",
	abstract = {Addressing online harms requires understanding the role of recommender systems for digital platforms.},
	language = {en-US},
	urldate = {2023-01-06},
	journal = {Brookings},
	author = {Meserole, Chris},
	month = sep,
	year = {2022},
 }
@misc{albrechtsen_guide_2020,
	title = {Guide: {Hvad} er desk research, og hvordan kan du gribe det an?},
	shorttitle = {Guide},
	url = {https://tovejs.dk/2020/02/19/desk-research-2/},
	howpublished ="\url{https://tovejs.dk/2020/02/19/desk-research-2/}",
	abstract = {Gennem desk research finder du frem til den viden, der allerede findes om dit emne. Læs om, hvordan du griber desk research an gennem 4 trin.},
	language = {da-DK},
	urldate = {2023-01-06},
	journal = {Tovejs},
	author = {Albrechtsen, Charlotte},
	month = feb,
	year = {2020},
}

@misc{roth_resource-efficient_2022,
	title = {Resource-{Efficient} {Neural} {Networks} for {Embedded} {Systems}},
	url = {http://arxiv.org/abs/2001.03048},
	abstract = {While machine learning is traditionally a resource intensive task, embedded systems, autonomous navigation, and the vision of the Internet of Things fuel the interest in resource-efficient approaches. These approaches aim for a carefully chosen trade-off between performance and resource consumption in terms of computation and energy. The development of such approaches is among the major challenges in current machine learning research and key to ensure a smooth transition of machine learning technology from a scientific environment with virtually unlimited computing resources into everyday's applications. In this article, we provide an overview of the current state of the art of machine learning techniques facilitating these real-world requirements. In particular, we focus on deep neural networks (DNNs), the predominant machine learning models of the past decade. We give a comprehensive overview of the vast literature that can be mainly split into three non-mutually exclusive categories: (i) quantized neural networks, (ii) network pruning, and (iii) structural efficiency. These techniques can be applied during training or as post-processing, and they are widely used to reduce the computational demands in terms of memory footprint, inference speed, and energy efficiency. We also briefly discuss different concepts of embedded hardware for DNNs and their compatibility with machine learning techniques as well as potential for energy and latency reduction. We substantiate our discussion with experiments on well-known benchmark datasets using compression techniques (quantization, pruning) for a set of resource-constrained embedded systems, such as CPUs, GPUs and FPGAs. The obtained results highlight the difficulty of finding good trade-offs between resource efficiency and predictive performance.},
	language = {en},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Roth, Wolfgang and Schindler, Günther and Klein, Bernhard and Peharz, Robert and Tschiatschek, Sebastian and Fröning, Holger and Pernkopf, Franz and Ghahramani, Zoubin},
	month = dec,
	year = {2022},
	note = {arXiv:2001.03048 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1812.02240},
}

@misc{goldstein_how_2022,
	type = {Tweet},
	title = {How many {GPUs} does it take to run {ChatGPT}? {And} how expensive is it for {OpenAI}? {Let}’s find out! ����},
	url = {https://twitter.com/tomgoldsteincs/status/1600196981955100694},
	howpublished = "\url{https://twitter.com/tomgoldsteincs/status/1600196981955100694}",
	language = {en},
	urldate = {2023-01-06},
	journal = {Twitter},
	author = {Goldstein, Thomas},
	month = dec,
	year = {2022},
 }

 
@inproceedings{malawski_benchmarking_2018-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Benchmarking {Heterogeneous} {Cloud} {Functions}},
	isbn = {978-3-319-75178-8},
	doi = {10.1007/978-3-319-75178-8_34},
	abstract = {Cloud Functions, often called Function-as-a-Service (FaaS), pioneered by AWS Lambda, are an increasingly popular method of running distributed applications. As in other cloud offerings, cloud functions are heterogeneous, due to different underlying hardware, runtime systems, as well as resource management and billing models. In this paper, we focus on performance evaluation of cloud functions, taking into account heterogeneity aspects. We developed a cloud function benchmarking framework, consisting of one suite based on Serverless Framework, and one based on HyperFlow. We deployed the CPU-intensive benchmarks: Mersenne Twister and Linpack, and evaluated all the major cloud function providers: AWS Lambda, Azure Functions, Google Cloud Functions and IBM OpenWhisk. We make our results available online and continuously updated. We report on the initial results of the performance evaluation and we discuss the discovered insights on the resource allocation policies.},
	language = {en},
	booktitle = {Euro-{Par} 2017: {Parallel} {Processing} {Workshops}},
	publisher = {Springer International Publishing},
	author = {Malawski, Maciej and Figiela, Kamil and Gajek, Adam and Zima, Adam},
	editor = {Heras, Dora B. and Bougé, Luc and Mencagli, Gabriele and Jeannot, Emmanuel and Sakellariou, Rizos and Badia, Rosa M. and Barbosa, Jorge G. and Ricci, Laura and Scott, Stephen L. and Lankes, Stefan and Weidendorfer, Josef},
	year = {2018},
	keywords = {Cloud computing, Cloud functions, FaaS, Performance evaluation},
	pages = {415--426},

 }

@misc{amazon_using_nodate-1,
	title = {Using the {AWS} {SDK} for {Rust} in {AWS} {Lambda} - {AWS} {SDK} for {Rust}},
	url = {https://docs.aws.amazon.com/sdk-for-rust/latest/dg/lambda.html#lambda-step3},
	howpublished ="\url{https://docs.aws.amazon.com/sdk-for-rust/latest/dg/lambda.html#lambda-step3}",
	urldate = {2023-01-06},
 year = {2022},
	author = {{Amazon}},
 }
@techreport{melnikov_websocket_2011,
	type = {Request for {Comments}},
	title = {The {WebSocket} {Protocol}},
	url = {https://datatracker.ietf.org/doc/rfc6455},
	howpublished = "\url{https://datatracker.ietf.org/doc/rfc6455}",
	abstract = {The WebSocket Protocol enables two-way communication between a client running untrusted code in a controlled environment to a remote host that has opted-in to communications from that code. The security model used for this is the origin-based security model commonly used by web browsers. The protocol consists of an opening handshake followed by basic message framing, layered over TCP. The goal of this technology is to provide a mechanism for browser-based applications that need two-way communication with servers that does not rely on opening multiple HTTP connections (e.g., using XMLHttpRequest or {\textbackslash}textlessiframe{\textbackslash}textgreaters and long polling). [STANDARDS-TRACK]},
	number = {RFC 6455},
	urldate = {2023-01-06},
	institution = {Internet Engineering Task Force},
	author = {Melnikov, Alexey and Fette, Ian},
	month = dec,
	year = {2011},
	doi = {10.17487/RFC6455},
	note = {Num Pages: 71},
}

@misc{lucidchart_how_2021,
	title = {How to {Draw} 5 {Types} of {Architectural} {Diagrams}},
	url = {https://www.lucidchart.com/blog/how-to-draw-architectural-diagrams},
	howpublished = "\url{https://www.lucidchart.com/blog/how-to-draw-architectural-diagrams}",
	abstract = {Let’s discuss the different types of architectural diagrams and the purpose that each serves.},
	language = {en},
	urldate = {2023-01-06},
	journal = {Lucidchart},
	author = {{Lucidchart}},
	month = apr,
	year = {2021},
}

@inproceedings{ford_chat_2017,booktitle={{}},
	title = {Chat {Speed} {OP} {PogChamp}: {Practices} of {Coherence} in {Massive} {Twitch} {Chat}},
	shorttitle = {Chat {Speed} {OP} {PogChamp}},
	doi = {10.1145/3027063.3052765},
	abstract = {Twitch.tv, a streaming platform known for video game content, has grown tremendously since its inception in 2011. We examine communication practices in Twitch chats for the popular game Hearthstone, comparing massive chats with at least 10,000 concurrent viewers and small chats with fewer than 2,000 concurrent viewers. Due to the large scale and fast pace of massive chats, communication patterns no longer follow models developed in previous studies of computer-mediated communication. Rather than what other studies have described as communication breakdowns and information overload, participants in massive chats communicate in what we call 'crowdspeak.'},
	author = {Ford, Colin and Gardner, Daniel and Horgan, Leah and Liu, Calvin and tsaasan, a and Nardi, Bonnie and Rickman, Jordan},
	month = may,
	year = {2017},
	pages = {858--871},
}

@misc{alexander_guide_2018,
	title = {A guide to understanding {Twitch} emotes},
	howpublished = "\url{https://www.polygon.com/2018/5/14/17335670/twitch-emotes-meaning-list-kappa-monkas-omegalul-pepe-trihard}",
	how = {https://www.polygon.com/2018/5/14/17335670/twitch-emotes-meaning-list-kappa-monkas-omegalul-pepe-trihard},
	abstract = {Some of Twitch’s most popular emotes, explained},
	language = {en-US},
	urldate = {2023-01-06},
	journal = {Polygon},
	author = {Alexander, Julia},
	month = may,
	year = {2018},
}

@misc{rust_what_nodate,
	type = {Documentation},
	title = {What is {Ownership}? - {The} {Rust} {Programming} {Language}},
	url = {https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html},
	howpublished="\url{https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html}",
	urldate = {2023-01-06},
	journal = {What is Ownership? - The Rust Programming Language},
	author = {{Rust}},
 	year = {2022},

 }


  @misc{fatima_2022, title={What's an ETL pipeline and how it's different from a data pipeline}, url={https://www.astera.com/type/blog/etl-pipeline-vs-data-pipeline/},


   howpublished="\url{https://www.astera.com/type/blog/etl-pipeline-vs-data-pipeline/}",
  
  journal={Astera}, author={Fatima, Nida}, year={2022}, month={Nov}} 

  